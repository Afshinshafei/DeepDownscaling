{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Configuration and Intercomparison of Deep Learning Neural Models for Statistical Downscaling \n",
    "### *Submitted to Geoscientific Model Development (October 2019)*\n",
    "### J. Baño-Medina, R. Manzanas and J. M. Gutiérrez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of the companion material to the manuscript entitled *Configuration and Intercomparison of Deep Learning Neural Models for Statistical Downscaling* by *J. Baño-Medina, R. Manzanas and J. M. Gutiérrez*, which has been submitted for discussion to *Geoscientific Model Development* in October 2019. In order to increase research transparency, this document allows to reproduce the results presented in the mentioned paper for one of the methods undertaken there, the prediction of temperature with a convolutional neural network (CNN hereafter) with 10 feature maps. Note however that the full code needed to reproduce the rest of experiments presented in the paper can be found in [GitHub?](). \n",
    "\n",
    "**Note:** This notebook (and the rest of the code delivered with the paper via [GitHub?]()) is written in the free programming language `R`, which has been increasingly used by data miners and climate scientits during the last years. Moreover, a great part of the code relies on [climate4R](http://www.meteo.unican.es/climate4R) (C4R hereafter), a suite of `R` packages developed by the [Santander Met Group](http://meteo.unican.es) for transparent climate data access, post processing (including bias correction and downscaling) and visualization. The interested reader is referred to [Iturbide et al. 2019](https://www.sciencedirect.com/science/article/pii/S1364815218303049?via%3Dihub) for further details on C4R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Setting working environment\n",
    "First of all, we define the directory in which the data, models and figures that we are going to create will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wd = \"/home/rodri/work/tmp/CNN\"  # working directory \n",
    "var = \"temperature\"  # variable of interest (just a label)\n",
    "\n",
    "## output directories\n",
    "dir.data = paste0(wd, \"/data/\", var)\n",
    "dir.create(dir.data, recursive = TRUE, showWarnings = FALSE)\n",
    "dir.models = paste0(wd, \"/models/\", var)\n",
    "dir.create(dir.models, recursive = TRUE, showWarnings = FALSE)\n",
    "dir.figs = paste0(wd, \"/figs/\", var)\n",
    "dir.create(dir.figs, recursive = TRUE, showWarnings = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Loading and preparing the data\n",
    "We need now to load the climate data which will be used hereafter, both the predictors and the precictand. As explained in the paper, we have considered 20 large-scale variables from the ERA-Interim reanalysis as predictors and surface temperature from E-OBS as predictand. All these data can be loaded from the User Data Getaway-Thredds Access Portal ([UDG-TAP](http://meteo.unican.es/udg-tap/home)) from the Santander Met Group, which provides access to various kinds of climate datasets (global and regional climate models, reanalysis, observations...). We will use the `loadGridData` from the [`loadeR`](https://github.com/SantanderMetGroup/loadeR) package (included in C4R) to read the data. The package [`transformeR`](https://github.com/SantanderMetGroup/transformeR) (included in C4R) will be also needed to perform basic operations (data splitting, collocation, etc.) on the C4R grid objects.\n",
    "\n",
    "**Note:** A registered user in the UDG-TAP is needed to run this step (registration can be done [here](http://meteo.unican.es/udg-tap/signup))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-09-24 20:11:22] Setting credentials...\n",
      "[2019-09-24 20:11:22] Success!\n",
      "Go to <http://www.meteo.unican.es/udg-tap/home> for details on your authorized groups and datasets\n",
      "transformeR version 1.5.1 (2019-07-13) is loaded\n",
      "Please see 'citation(\"transformeR\")' to cite this package.\n"
     ]
    }
   ],
   "source": [
    "library(loadeR)  # package containing the function \"loadGridData\"\n",
    "loginUDG(username = \"\", password = \"\")  # to access the datasets it is first needed to log into the UDG-TAP\n",
    "library(transformeR)  # package to perform basic operations on C4R grid objects\n",
    "library(magrittr)  # this library is useful to simplify the code syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Loading predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:11:33] Defining harmonization parameters for variable \"z@500\"\n",
      "[2019-09-24 20:11:33] Opening dataset...\n",
      "[2019-09-24 20:11:34] The dataset was successfuly opened\n",
      "[2019-09-24 20:11:34] Defining geo-location parameters\n",
      "[2019-09-24 20:11:35] Defining time selection parameters\n",
      "[2019-09-24 20:11:35] Retrieving data subset ...\n",
      "[2019-09-24 20:11:50] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:11:50] Defining harmonization parameters for variable \"z@700\"\n",
      "[2019-09-24 20:11:50] Opening dataset...\n",
      "[2019-09-24 20:11:50] The dataset was successfuly opened\n",
      "[2019-09-24 20:11:51] Defining geo-location parameters\n",
      "[2019-09-24 20:11:51] Defining time selection parameters\n",
      "[2019-09-24 20:11:51] Retrieving data subset ...\n",
      "[2019-09-24 20:12:07] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:12:07] Defining harmonization parameters for variable \"z@850\"\n",
      "[2019-09-24 20:12:07] Opening dataset...\n",
      "[2019-09-24 20:12:08] The dataset was successfuly opened\n",
      "[2019-09-24 20:12:08] Defining geo-location parameters\n",
      "[2019-09-24 20:12:08] Defining time selection parameters\n",
      "[2019-09-24 20:12:08] Retrieving data subset ...\n",
      "[2019-09-24 20:12:24] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:12:24] Defining harmonization parameters for variable \"z@1000\"\n",
      "[2019-09-24 20:12:24] Opening dataset...\n",
      "[2019-09-24 20:12:24] The dataset was successfuly opened\n",
      "[2019-09-24 20:12:24] Defining geo-location parameters\n",
      "[2019-09-24 20:12:24] Defining time selection parameters\n",
      "[2019-09-24 20:12:24] Retrieving data subset ...\n",
      "[2019-09-24 20:12:40] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:12:40] Defining harmonization parameters for variable \"hus@500\"\n",
      "[2019-09-24 20:12:40] Opening dataset...\n",
      "[2019-09-24 20:12:40] The dataset was successfuly opened\n",
      "[2019-09-24 20:12:40] Defining geo-location parameters\n",
      "[2019-09-24 20:12:40] Defining time selection parameters\n",
      "[2019-09-24 20:12:40] Retrieving data subset ...\n",
      "[2019-09-24 20:12:56] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:12:56] Defining harmonization parameters for variable \"hus@700\"\n",
      "[2019-09-24 20:12:56] Opening dataset...\n",
      "[2019-09-24 20:12:56] The dataset was successfuly opened\n",
      "[2019-09-24 20:12:56] Defining geo-location parameters\n",
      "[2019-09-24 20:12:56] Defining time selection parameters\n",
      "[2019-09-24 20:12:56] Retrieving data subset ...\n",
      "[2019-09-24 20:13:11] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:13:11] Defining harmonization parameters for variable \"hus@850\"\n",
      "[2019-09-24 20:13:11] Opening dataset...\n",
      "[2019-09-24 20:13:11] The dataset was successfuly opened\n",
      "[2019-09-24 20:13:11] Defining geo-location parameters\n",
      "[2019-09-24 20:13:11] Defining time selection parameters\n",
      "[2019-09-24 20:13:11] Retrieving data subset ...\n",
      "[2019-09-24 20:13:27] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:13:27] Defining harmonization parameters for variable \"hus@1000\"\n",
      "[2019-09-24 20:13:27] Opening dataset...\n",
      "[2019-09-24 20:13:27] The dataset was successfuly opened\n",
      "[2019-09-24 20:13:27] Defining geo-location parameters\n",
      "[2019-09-24 20:13:28] Defining time selection parameters\n",
      "[2019-09-24 20:13:28] Retrieving data subset ...\n",
      "[2019-09-24 20:13:47] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:13:47] Defining harmonization parameters for variable \"ta@500\"\n",
      "[2019-09-24 20:13:47] Opening dataset...\n",
      "[2019-09-24 20:13:47] The dataset was successfuly opened\n",
      "[2019-09-24 20:13:47] Defining geo-location parameters\n",
      "[2019-09-24 20:13:47] Defining time selection parameters\n",
      "[2019-09-24 20:13:47] Retrieving data subset ...\n",
      "[2019-09-24 20:14:04] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:14:04] Defining harmonization parameters for variable \"ta@700\"\n",
      "[2019-09-24 20:14:04] Opening dataset...\n",
      "[2019-09-24 20:14:04] The dataset was successfuly opened\n",
      "[2019-09-24 20:14:04] Defining geo-location parameters\n",
      "[2019-09-24 20:14:04] Defining time selection parameters\n",
      "[2019-09-24 20:14:04] Retrieving data subset ...\n",
      "[2019-09-24 20:14:22] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:14:22] Defining harmonization parameters for variable \"ta@850\"\n",
      "[2019-09-24 20:14:22] Opening dataset...\n",
      "[2019-09-24 20:14:22] The dataset was successfuly opened\n",
      "[2019-09-24 20:14:22] Defining geo-location parameters\n",
      "[2019-09-24 20:14:22] Defining time selection parameters\n",
      "[2019-09-24 20:14:22] Retrieving data subset ...\n",
      "[2019-09-24 20:14:39] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:14:39] Defining harmonization parameters for variable \"ta@1000\"\n",
      "[2019-09-24 20:14:39] Opening dataset...\n",
      "[2019-09-24 20:14:39] The dataset was successfuly opened\n",
      "[2019-09-24 20:14:39] Defining geo-location parameters\n",
      "[2019-09-24 20:14:39] Defining time selection parameters\n",
      "[2019-09-24 20:14:39] Retrieving data subset ...\n",
      "[2019-09-24 20:14:56] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:14:56] Defining harmonization parameters for variable \"ua@500\"\n",
      "[2019-09-24 20:14:56] Opening dataset...\n",
      "[2019-09-24 20:14:56] The dataset was successfuly opened\n",
      "[2019-09-24 20:14:56] Defining geo-location parameters\n",
      "[2019-09-24 20:14:56] Defining time selection parameters\n",
      "[2019-09-24 20:14:56] Retrieving data subset ...\n",
      "[2019-09-24 20:15:12] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:15:12] Defining harmonization parameters for variable \"ua@700\"\n",
      "[2019-09-24 20:15:12] Opening dataset...\n",
      "[2019-09-24 20:15:12] The dataset was successfuly opened\n",
      "[2019-09-24 20:15:12] Defining geo-location parameters\n",
      "[2019-09-24 20:15:12] Defining time selection parameters\n",
      "[2019-09-24 20:15:12] Retrieving data subset ...\n",
      "[2019-09-24 20:15:29] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:15:29] Defining harmonization parameters for variable \"ua@850\"\n",
      "[2019-09-24 20:15:29] Opening dataset...\n",
      "[2019-09-24 20:15:29] The dataset was successfuly opened\n",
      "[2019-09-24 20:15:29] Defining geo-location parameters\n",
      "[2019-09-24 20:15:29] Defining time selection parameters\n",
      "[2019-09-24 20:15:29] Retrieving data subset ...\n",
      "[2019-09-24 20:15:45] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:15:45] Defining harmonization parameters for variable \"ua@1000\"\n",
      "[2019-09-24 20:15:45] Opening dataset...\n",
      "[2019-09-24 20:15:46] The dataset was successfuly opened\n",
      "[2019-09-24 20:15:46] Defining geo-location parameters\n",
      "[2019-09-24 20:15:46] Defining time selection parameters\n",
      "[2019-09-24 20:15:46] Retrieving data subset ...\n",
      "[2019-09-24 20:16:04] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:16:04] Defining harmonization parameters for variable \"va@500\"\n",
      "[2019-09-24 20:16:04] Opening dataset...\n",
      "[2019-09-24 20:16:04] The dataset was successfuly opened\n",
      "[2019-09-24 20:16:04] Defining geo-location parameters\n",
      "[2019-09-24 20:16:04] Defining time selection parameters\n",
      "[2019-09-24 20:16:04] Retrieving data subset ...\n",
      "[2019-09-24 20:16:21] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:16:21] Defining harmonization parameters for variable \"va@700\"\n",
      "[2019-09-24 20:16:21] Opening dataset...\n",
      "[2019-09-24 20:16:21] The dataset was successfuly opened\n",
      "[2019-09-24 20:16:21] Defining geo-location parameters\n",
      "[2019-09-24 20:16:21] Defining time selection parameters\n",
      "[2019-09-24 20:16:21] Retrieving data subset ...\n",
      "[2019-09-24 20:16:37] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:16:37] Defining harmonization parameters for variable \"va@850\"\n",
      "[2019-09-24 20:16:38] Opening dataset...\n",
      "[2019-09-24 20:16:38] The dataset was successfuly opened\n",
      "[2019-09-24 20:16:38] Defining geo-location parameters\n",
      "[2019-09-24 20:16:38] Defining time selection parameters\n",
      "[2019-09-24 20:16:38] Retrieving data subset ...\n",
      "[2019-09-24 20:16:54] Done\n",
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:16:54] Defining harmonization parameters for variable \"va@1000\"\n",
      "[2019-09-24 20:16:54] Opening dataset...\n",
      "[2019-09-24 20:16:54] The dataset was successfuly opened\n",
      "[2019-09-24 20:16:54] Defining geo-location parameters\n",
      "[2019-09-24 20:16:55] Defining time selection parameters\n",
      "[2019-09-24 20:16:55] Retrieving data subset ...\n",
      "[2019-09-24 20:17:10] Done\n"
     ]
    }
   ],
   "source": [
    "## --- the next chunk of code takes about 5 minutes to be completed \n",
    "## --- in a personal computer with the technical specifications given at the end of this notebook\n",
    "\n",
    "## loading predictor variables from ERA-Interim\n",
    "variables <- c(\"z@500\",\"z@700\",\"z@850\",\"z@1000\",  \n",
    "               \"hus@500\",\"hus@700\",\"hus@850\",\"hus@1000\",\n",
    "               \"ta@500\",\"ta@700\",\"ta@850\",\"ta@1000\",\n",
    "               \"ua@500\",\"ua@700\",\"ua@850\",\"ua@1000\",\n",
    "               \"va@500\",\"va@700\",\"va@850\",\"va@1000\")\n",
    "x <- lapply(variables, function(x) {\n",
    "  loadGridData(dataset = \"ECMWF_ERA-Interim-ESD\",\n",
    "               var = x,\n",
    "               lonLim = c(-10, 32), # longitude domain: 22 gridboxes (2º resolution)\n",
    "               latLim = c(36, 72),  # latitude domain: 19 gridboxes (2º resolution)\n",
    "               years = 1979:2008)  # total period of study\n",
    "}) %>% makeMultiGrid() %>% redim(, drop = T)\n",
    "## saving predictor data\n",
    "save(x, file = paste0(dir.data, \"/x.rda\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Loading predictand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Accessing harmonized data from a public UDG dataset\n",
      "[2019-09-24 20:19:05] Defining harmonization parameters for variable \"tas\"\n",
      "[2019-09-24 20:19:05] Opening dataset...\n",
      "[2019-09-24 20:19:05] The dataset was successfuly opened\n",
      "[2019-09-24 20:19:05] Defining geo-location parameters\n",
      "[2019-09-24 20:19:05] Defining time selection parameters\n",
      "[2019-09-24 20:19:06] Retrieving data subset ...\n",
      "[2019-09-24 20:20:02] Done\n"
     ]
    }
   ],
   "source": [
    "## --- the next chunk of code takes about 1 minute to be completed \n",
    "## --- in a personal computer with the technical specifications given at the end of this notebook\n",
    "\n",
    "# loading predictand (temperature), from E-OBS\n",
    "y <- loadGridData(dataset = \"E-OBS_v14_0.50regular\",\n",
    "                   var = \"tas\",\n",
    "                   lonLim = c(-10, 32), # longitude domain: 85 gridboxes (0.5º resolution)\n",
    "                   latLim = c(36, 72),  # latitude domain: 73 gridboxes (0.5º resolution)\n",
    "                   years = 1979:2008)  # total period of study\n",
    "# saving predictand data\n",
    "save(y, file = paste0(dir.data, \"/y.rda\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Train/test partition\n",
    "As in the paper, we split the entire period of study into *train* (1979-2002) and *test* (2003-2008). To do that, we use the `subsetGrid` function from `transformeR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## predictor and predictand data for the train period\n",
    "xT <- subsetGrid(x, years = 1979:2002)\n",
    "yT <- subsetGrid(y, years = 1979:2002)\n",
    "\n",
    "## predictor and predictand data for the test period\n",
    "xt <- subsetGrid(x ,years = 2003:2008)\n",
    "yt <- subsetGrid(y ,years = 2003:2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Data standardization\n",
    "The predictors are then standardized at a gridbox level using the function `scaleGrid` (also from `transformeR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-09-24 20:22:08] - Scaling ...\n",
      "[2019-09-24 20:22:24] - Done\n",
      "[2019-09-24 20:22:24] - Scaling ...\n",
      "[2019-09-24 20:22:38] - Done\n"
     ]
    }
   ],
   "source": [
    "## standardazing predictor data\n",
    "xT <- scaleGrid(xT, type = \"standardize\", spatial.frame = \"gridbox\") %>% redim(drop = TRUE)  # train period\n",
    "xt <- scaleGrid(xt, xT, type = \"standardize\", spatial.frame = \"gridbox\") %>% redim(drop = TRUE)  # test period: standardization is done with respect to the train period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Downscaling\n",
    "Once the predictors and the predictand are loaded, we can start the downscaling process. In this example we will use the [`keras`](https://cran.r-project.org/web/packages/keras/index.html) library, which provides an `R` interface to [Keras](https://keras.io), a high-level neural networks API which supports arbitrary network architectures and is seamlessly integrated with [TensorFlow](https://www.tensorflow.org/) (note that Keras and TensorFlow are the state of the art in deep learning tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "library(keras)  # loading keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For simplicity, we define an auxiliary function called `C4R2Keras` which converts our climate data (C4R grid objects) to the format required to easily work with `keras`. In particular, this function discards all the gridboxes over the sea in the E-OBS dataset, for which there are no data; and also those land points for which some missing data is present (`keras` does not support missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# auxiliary function to convert C4R data to the format required by keras\n",
    "C4R2Keras <- function(x, y) {\n",
    "  xx <- x$Data\n",
    "  xx <- xx %>% aperm(c(2,3,4,1))  # time*lat*lon*var (for convenience when using Keras built-in functions)\n",
    "  yy <- y$Data\n",
    "  dim(yy) <- c(dim(yy)[1], dim(yy)[2]*dim(yy)[3])\n",
    "  indLand <- (!apply(yy, MARGIN = 2, anyNA)) %>% which()\n",
    "  yy <- yy[, indLand]  # keeping only land points with no missing values\n",
    "  \n",
    "  data.Keras = list()\n",
    "  data.Keras$x = xx\n",
    "  data.Keras$y = yy\n",
    "  return(data.Keras)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Network architecture\n",
    "Next, we use `keras` to define the architecture of our network (labelled as CNN10 in the paper; see Table 2). This network consists on a block of three convolutional layers with 50, 25 and 10 (3 x 3 x number of inputs coming from the previous layer) kernels, respectively. In the hidden layers, the *ReLu* activation function is used, and padding is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## defining the network architecture in keras\n",
    "architecture = \"CNN10\"  # just a label\n",
    "callbacks <- list(callback_early_stopping(patience = 30),  # we fix patience = 30 (based on empirical screening)\n",
    "                  callback_model_checkpoint(filepath = paste0(dir.models, \"/\", architecture, \".h5\"),\n",
    "                                            monitor = 'val_loss', save_best_only = TRUE)  # we save only the best model (the one with the smallest validation error)\n",
    ")\n",
    "\n",
    "dataT.Keras <- C4R2Keras(xT, yT)  # predictor and predictand data for the train period\n",
    "inputs <- layer_input(shape = dim(dataT.Keras$x)[-1])  # inputs\n",
    "l1 = layer_conv_2d(inputs, filters = 50, kernel_size = c(3, 3), activation = 'relu', padding = \"valid\")   # first layer\n",
    "l2 = layer_conv_2d(l1, filters = 25, kernel_size = c(3, 3), activation = 'relu', padding = \"valid\")   # second layer\n",
    "l3 = layer_conv_2d(l2, filters = 10, kernel_size = c(3, 3), activation = 'relu', padding = \"valid\")   # third layer\n",
    "l4 = layer_flatten(l3)  # fourth layer\n",
    "outputs <- layer_dense(l4, units = ncol(dataT.Keras$y))  # outputs\n",
    "model <- keras_model(inputs = inputs, outputs = outputs)  # network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Training the network\n",
    "We are now in the position to compile and train the network. Note that, in terms of computation, this is the most expensive step (it took 3 hours in the machine used to write this notebook). Despite the parameter epochs is set to 10000 we perform an early-stopping criteria and therefore the training ends whenever the error in a validation dataset (10% of data as validation_split = 0.1) stops decreasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## --- the next chunk of code takes about 3 hours to be completed \n",
    "## --- in a personal computer with the technical specifications given at the end of this notebook\n",
    "\n",
    "## compiling and fitting the network in keras (the best model is automatically saved)\n",
    "model %>% compile(optimizer = optimizer_adam(lr = 0.0001), loss = \"mse\")   # loss function: \"MSE\"\n",
    "model %>% fit(dataT.Keras$x, dataT.Keras$y, epochs = 10000, batch_size = 100, \n",
    "              validation_split = 0.1, callbacks = callbacks, verbose = 1)  \n",
    "k_clear_session()  # it is important to clear the session right after the model is fitted\n",
    "\n",
    "## --- stopped at epoch 431, with loss: 0.8510, and val_loss: 1.0944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Making predictions\n",
    "Once the network has been successfully trained, we can use it to make our predictions. Before doing so, we create a pair of auxiliary functions, `convert2map2D` and `predictDEEP`. The latter loads the model that thas been previously fitted, makes the predicions and relies on the former to reconstruct these predictions to a 2-D map (in which missing values are treated conveniently), saving them as a C4R grid object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## auxiliary function to convert the predictions returned by keras to a lat/lon regular 2D-grid, filling with NA points for which there are no predictions (e.g. over the sea)\n",
    "convert2map2D <- function(data, template, templateT) {\n",
    "  ntime <- dim(template$Data)[1]\n",
    "  nlat <- dim(template$Data)[2]\n",
    "  nlon <- dim(template$Data)[3]\n",
    "  \n",
    "  data.aux <- templateT$Data\n",
    "  dim(data.aux) <- c(dim(data.aux)[1], nlat*nlon)\n",
    "  indLand <- (!apply(data.aux, MARGIN = 2, anyNA)) %>% which()\n",
    "  indSea <- (apply(data.aux, MARGIN = 2,anyNA)) %>% which()\n",
    "  \n",
    "  out <- template\n",
    "  dim(out$Data) <- c(ntime, nlat*nlon)\n",
    "  out$Data[, indLand] <- data\n",
    "  out$Data[, indSea] <- NA\n",
    "  dim(out$Data) <- c(ntime, nlat, nlon)\n",
    "  return(out)\n",
    "}\n",
    "\n",
    "## auxiliary function used to predict (we define this function for convenience)\n",
    "predictDEEP <- function(x, template, templateT, architecture, dataset) {\n",
    "  model <- load_model_hdf5(filepath = paste0(dir.models, \"/\", architecture, \".h5\"))  # loading the model that has been previously saved\n",
    "  \n",
    "  pred <- model$predict(C4R2Keras(x, template)$x)  # predictions returned by keras (only for land points with no missing values)\n",
    "  pred <- convert2map2D(pred, template, templateT)\n",
    "  \n",
    "  save(pred,\n",
    "       file = paste0(dir.data, \"/predictions_\", dataset, \"_\", architecture, \".rda\"))\n",
    "  k_clear_session()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "At this point, we use `predictDEEP` to create and save the predicions for the test period (2003-2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## --- the next chunk of code takes about 6 seconds to be completed \n",
    "## --- in a personal computer with the technical specifications given at the end of this notebook\n",
    "\n",
    "## making predictions for the test period\n",
    "predictDEEP(xt, template = yt, templateT = yT, architecture = \"CNN10\", dataset = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Validation of results\n",
    "Finally, the predictions which have been just created (and saved) are loaded and validated. Here we focus on the mean error obtained with respect to the observed cimatological value (bias) and the observed extreme percentiles 2th and 98th (biasP02 and biasP98). These metrics are computed using the function `valueMeasure` from the [`climate4R.value`](https://github.com/SantanderMetGroup/climate4R.value) package, a wrapper to the [`VALUE`](https://github.com/SantanderMetGroup/VALUE) package, which was developed in the [COST action VALUE](http://www.value-cost.eu/) for validation purposes. To plot the final maps (comparable to those shown in the bottom row of Figure 5 in the paper) we also need to load the [`visualizeR`](https://github.com/SantanderMetGroup/visualizeR) and `RColorBrewer` packages. If interested in knowing more about the plotting options used here, the reader is referred to the documentation of the `spatialPlot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-09-24 20:26:00] Computing member 1 out of 1\n",
      "[2019-09-24 20:26:13] Done.\n",
      "[2019-09-24 20:26:14] Computing member 1 out of 1\n",
      "[2019-09-24 20:26:27] Done.\n",
      "[2019-09-24 20:26:28] Computing member 1 out of 1\n",
      "[2019-09-24 20:26:41] Done.\n",
      "[2019-09-24 20:26:42] Computing member 1 out of 1\n",
      "[2019-09-24 20:26:58] Done.\n",
      "[2019-09-24 20:26:59] Computing member 1 out of 1\n",
      "[2019-09-24 20:27:15] Done.\n",
      "[2019-09-24 20:27:16] Computing member 1 out of 1\n",
      "[2019-09-24 20:27:30] Done.\n",
      "[2019-09-24 20:27:31] Computing member 1 out of 1\n",
      "[2019-09-24 20:27:50] Done.\n",
      "[2019-09-24 20:27:51] Computing member 1 out of 1\n",
      "[2019-09-24 20:28:12] Done.\n",
      "[2019-09-24 20:28:13] Computing member 1 out of 1\n",
      "[2019-09-24 20:28:25] Done.\n",
      "[2019-09-24 20:28:25] - Computing climatology...\n",
      "[2019-09-24 20:28:26] - Done.\n",
      "[2019-09-24 20:28:26] - Computing climatology...\n",
      "[2019-09-24 20:28:26] - Done.\n",
      "[2019-09-24 20:28:26] - Computing climatology...\n",
      "[2019-09-24 20:28:26] - Done.\n"
     ]
    }
   ],
   "source": [
    "## --- the next chunk of code takes about 2 minutes be completed \n",
    "## --- in a personal computer with the technical specifications given at the end of this notebook\n",
    "\n",
    "## loading packages\n",
    "library(climate4R.value)\n",
    "library(visualizeR)\n",
    "library(RColorBrewer)  # for color palettes\n",
    "\n",
    "load(paste0(dir.data, \"/predictions_test_\", architecture, \".rda\"))  # loading prediction (previously obtained)\n",
    "\n",
    "## computing validation metrics\n",
    "bias <- valueMeasure(yt, pred, measure.code = \"bias\", index.code = \"Mean\")$Measure %>% redim()\n",
    "biasP02 <- valueMeasure(yt, pred, measure.code = \"bias\", index.code = \"P02\")$Measure %>% redim()\n",
    "biasP98 <- valueMeasure(yt, pred, measure.code = \"bias\", index.code = \"P98\")$Measure %>% redim()\n",
    "\n",
    "## producing (and saving) final figure, in pdf format\n",
    "#pdf(file = paste0(dir.figs, \"/\", architecture, \"_bias_test.pdf\"))  # output file. uncomment this line if wants to directly save the figure to a specific location\n",
    "spatialPlot(makeMultiGrid(climatology(bias),\n",
    "                          climatology(biasP02),\n",
    "                          climatology(biasP98)),  \n",
    "            backdrop.theme = \"coastline\",\n",
    "            layout = c(3, 1), as.table = T,\n",
    "            main = architecture,\n",
    "            names.attr = c(\"bias\", \"bias (P02)\", \"bias (P98)\"),\n",
    "            col.regions = colorRampPalette(rev(brewer.pal(n = 9, \"RdBu\"))),\n",
    "            set.min = -2, set.max = 2, at = seq(-2, 2, 0.1))\n",
    "#dev.off()   # uncomment this line if wants to directly save the figure to a specific location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Session info\n",
    "Finally, we show the technical specifications of the machine in which this notebook has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R version 3.6.1 (2019-07-05)\n",
      "Platform: x86_64-pc-linux-gnu (64-bit)\n",
      "Running under: Ubuntu 18.04.3 LTS\n",
      "\n",
      "Matrix products: default\n",
      "BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1\n",
      "LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1\n",
      "\n",
      "locale:\n",
      " [1] LC_CTYPE=es_ES.UTF-8          LC_NUMERIC=C                 \n",
      " [3] LC_TIME=es_ES.UTF-8           LC_COLLATE=es_ES.UTF-8       \n",
      " [5] LC_MONETARY=es_ES.UTF-8       LC_MESSAGES=es_ES.UTF-8      \n",
      " [7] LC_PAPER=es_ES.UTF-8          LC_NAME=es_ES.UTF-8          \n",
      " [9] LC_ADDRESS=es_ES.UTF-8        LC_TELEPHONE=es_ES.UTF-8     \n",
      "[11] LC_MEASUREMENT=es_ES.UTF-8    LC_IDENTIFICATION=es_ES.UTF-8\n",
      "\n",
      "attached base packages:\n",
      "[1] stats     graphics  grDevices utils     datasets  methods   base     \n",
      "\n",
      "other attached packages:\n",
      "[1] magrittr_1.5      loadeR_1.4.15     loadeR.java_1.1.1 rJava_0.9-11     \n",
      "\n",
      "loaded via a namespace (and not attached):\n",
      " [1] Rcpp_1.0.2          digest_0.6.19       crayon_1.3.4       \n",
      " [4] bitops_1.0-6        IRdisplay_0.7.0     repr_1.0.1         \n",
      " [7] jsonlite_1.6        evaluate_0.13       pillar_1.4.0       \n",
      "[10] rlang_0.4.0         uuid_0.1-2          IRkernel_1.0.1.9000\n",
      "[13] tools_3.6.1         RCurl_1.95-4.12     abind_1.4-5        \n",
      "[16] compiler_3.6.1      base64enc_0.1-3     htmltools_0.3.6    \n",
      "[19] pbdZMQ_0.3-3       \n"
     ]
    }
   ],
   "source": [
    "print(sessionInfo())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
